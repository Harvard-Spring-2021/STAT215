write_tsv(data.frame(rownames(tail(res.df, 200))),
path = "i.8.txt")
# ranking all genes in the data set
gseaDat = res.df
ranks = res.df$log2FoldChange
names(ranks) = rownames(res.df)
barplot(sort(ranks, decreasing = T))
# loading in pathways data and correcting form
pathways = read.delim("Q1/c2.cp.kegg.v7.1.symbols.gmt",
header = FALSE, stringsAsFactors = FALSE, quote = "", sep = "\t")
pathways = subset(pathways, select = -V2)
pathways = t(pathways)
colnames(pathways) = pathways[1,]
pathways = pathways[-1,]
pathways = data.frame(pathways)
# running GSEA
fgseaRes <- fgsea(pathways, ranks, minSize=15, maxSize = 500, nperm=1000)
head(fgseaRes[order(padj, -abs(NES)), ], n=5)
plotEnrichment(pathways[["KEGG_CELL_CYCLE"]], ranks)
library(ggplot2)
library(ggfortify)
library(pROC)
library(caret)
# additional libraries
library(e1071)
library(MASS)
library(glmnet)
library(kernlab)
library(randomForest)
# read in data
train_label = read.delim("Q2/raw_data2/BRCA_phenotype.txt",
header = TRUE, stringsAsFactors = FALSE, quote = "", sep = "\t")
train_data = read.delim("Q2/raw_data2/BRCA_zscore_data.txt",
header = TRUE, stringsAsFactors = FALSE, quote = "", sep = "\t")
test_label = read.delim("Q2/raw_data2/diagnosis.txt",
header = TRUE, stringsAsFactors = FALSE, quote = "", sep = "\t")
test_data = read.delim("Q2/raw_data2/unknown_samples.txt",
header = TRUE, stringsAsFactors = FALSE, quote = "", sep = "\t")
### Your code here
# pca with prcomp command
res.pca = prcomp(train_data, scale = TRUE, center = TRUE)
# scree plot
pca.var.per = round(res.pca$sdev^2/sum(res.pca$sdev^2)*100,1)
fviz_eig(res.pca, addlabels=TRUE, ylim=c(0,100), geom = c("bar", "line"),
barfill = "gold", barcolor="grey", linecolor = "red", ncp=10) +
labs(title = "PCA Coverage",
x = "Principal Components", y = "% of variances")
View(train_data)
View(train_data)
# making pca plot data frame
pca_plot_data = data.frame(sample = rownames(res.pca$x),
X = res.pca$x[,1],
Y = res.pca$x[,2])
pca_plot_data = merge(pca_plot_data, train_label, by="sample")
# making pca plot
ggplot(data = pca_plot_data, aes(x=X, y=Y, label=sample, color=phenotype)) +
geom_point() +
xlab(paste("PC1: ",pca.var.per[1],"%",sep ="")) +
ylab(paste("PC2: ",pca.var.per[2],"%",sep ="")) +
theme_bw() +
ggtitle("PCA Graph")
### Your code here
pca.var = res.pca$sdev^2/sum(res.pca$sdev^2)
pca.var.acc = pca.var %>% accumulate(`+`)
min(which(pca.var.acc > 0.9))
plot(pca.var.acc, main="Cumulative % Variance Captured",
xlab="PC", ylab="Cumulative % Variance")
abline(h=0.9, col="red", lty=2)
abline(v=25, col="blue", lty=3)
### Your code here
set.seed(215)
# filter for the 25 top PCs
train_df = data.frame(res.pca$x[,c(1:25)])
train_outcome = data.frame(sample = rownames(train_df))
train_outcome = merge(train_outcome, train_label, by="sample")
train_outcome$phenotype = as.factor(train_outcome$phenotype)
# preProcessing for algorithms using Euclidean distances
scaled_train_df = predict(preProcess(train_df, method = c("center", "scale")),
train_df)
# fixed parameters
control = trainControl(method = "cv",
number = 5,
savePredictions = TRUE,
classProbs = TRUE)
metric = "Accuracy"
lambda = 10^seq(-3, 3, length = 100)
# KNN
knn = train(x = scaled_train_df,
y = train_outcome$phenotype,
method = "knn",
metric = metric,
trControl = control,
tuneGrid = expand.grid(k=seq(1,10,1)))
# Logistic Regression
# https://stackoverflow.com/questions/39550118/cross-validation-function-for-logistic-regression-in-r
lr = train(x = train_df,
y = train_outcome$phenotype,
method = "glm",
family = binomial(),
metric = metric,
trControl = control)
# Ridge Regression
ridge = train(x = train_df,
y = train_outcome$phenotype,
method = "glmnet",
trControl = control,
tuneGrid = expand.grid(alpha = 0, lambda = lambda))
# LASSO
lasso = train(x = train_df,
y = train_outcome$phenotype,
method = "glmnet",
trControl = control,
tuneGrid = expand.grid(alpha = 1, lambda = lambda))
# ElasticNet
elastic = train(x = train_df,
y = train_outcome$phenotype,
method = "glmnet",
trControl = control,
tuneLength = 10)
# Random Forest
rf = train(x = train_df,
y = train_outcome$phenotype,
method = "rf",
trControl = control,
tuneGrid = expand.grid(mtry=seq(1,15,1)))
# SVM
svm = train(x = train_df,
y = train_outcome$phenotype,
method = "svmLinear",
trControl = control,
tuneGrid = expand.grid(C=c(0.01, 0.1, 1, 10)))
### Your code here
# KNN summary
knn_best = knn[["bestTune"]]
knn_results = knn[["results"]]
knn_results[knn_results$k == knn_best$k, ]
# Logistic summary
lr[["results"]]
# Ridge summary
ridge_best = ridge[["bestTune"]]
ridge_results = ridge[["results"]]
ridge_results[ridge_results$lambda == ridge_best$lambda, ]
# Lasso summary
lasso_best = lasso[["bestTune"]]
lasso_results = lasso[["results"]]
lasso_results[lasso_results$lambda == lasso_best$lambda, ]
# Elastic summary
elastic_best = elastic[["bestTune"]]
elastic_results = elastic[["results"]]
elastic_results[elastic_results$lambda == elastic_best$lambda &
elastic_results$alpha == elastic_best$alpha, ]
# Random Forest summary
rf_best = rf[["bestTune"]]
rf_results = rf[["results"]]
rf_results[rf_results$mtry == rf_best$mtry,]
# SVM summary
svm_best = svm[["bestTune"]]
svm_results = svm[["results"]]
svm_results[svm_results$C == svm_best$C,]
### Your code here
# extracting coefficients from best models
lr_coef = coef(lr$finalModel)
ridge_coef = coef(ridge$finalModel, ridge$bestTune$lambda)
lasso_coef = coef(lasso$finalModel, lasso$bestTune$lambda)
elastic_coef = coef(elastic$finalModel, elastic$bestTune$lambda)
# printing out table of coefficients
coefs = data.frame(as.matrix(cbind(lr_coef, ridge_coef, lasso_coef, elastic_coef)))
colnames(coefs) = c("Logistic", "Ridge", "Lasso", "Elastic")
print(coefs)
### Your code here
# convert test to PCA
# Source: https://stats.stackexchange.com/questions/2592/how-to-project-a-new-vector-onto-pca-space
test_df = scale(test_data, res.pca$center, res.pca$scale) %*% res.pca$rotation
# keep top 25 PC
test_df = test_df[,c(1:25)]
# make prediction
predictions = elastic %>% predict(test_df)
predictions
### Your code here
# loading matrix
loading_mtx = abs(as.matrix(res.pca$rotation[,c(1:25)]))
# vector of coefficient
impt_vector = abs(as.matrix(coef(elastic$finalModel, elastic$bestTune$lambda)[-1]))
# contribution vector
contributions = loading_mtx %*% impt_vector
head(contributions[order(contributions, decreasing = TRUE), ], 3)
predictions
test_label
### Your code here
results = data.frame(sample = test_label$sample,
y = test_label$phenotype,
ypred = predictions)
results
### Your code here
# consolidated data frame of actual and predicted results
results = data.frame(sample = test_label$sample,
y = test_label$phenotype,
ypred = predictions)
# confusion matrix
confusionMatrix(results$y, results$ypred)
View(results)
View(results)
### Your code here
# consolidated data frame of actual and predicted results
results = data.frame(sample = test_label$sample,
y = as.factor(test_label$phenotype),
ypred = as.factor(predictions))
# confusion matrix
confusionMatrix(results$y, results$ypred)
# ROC curve
# prediction AUC
### Your code here
# consolidated data frame of actual and predicted results
results = data.frame(sample = test_label$sample,
y = as.factor(test_label$phenotype),
ypred = as.factor(predictions))
# confusion matrix
confusionMatrix(results$y, results$ypred)
knitr::opts_chunk$set(echo = TRUE)
library(ROCR)
#import the package
library(randomForest)
library(datasets)
data(iris)
# Perform training:
set.seed(17)
# Calculate the size of each of the data sets:
data_set_size <- floor(nrow(iris)/2)
# Generate a random sample of "data_set_size" indexes
indexes <- sample(1:nrow(iris), size = data_set_size)
# Assign the data to the correct sets
training <- iris[indexes,]
validation1 <- iris[-indexes,]
rf_classifier = randomForest(Species ~ ., data=training, ntree=100, mtry=2, importance=TRUE)
rf_classifier
prediction_for_table <- predict(rf_classifier,validation1[,-5])
table(observed=validation1[,5],predicted=prediction_for_table)
# Calculate the probability of new observations belonging to each class
# prediction_for_roc_curve will be a matrix with dimensions data_set_size x number_of_classes
prediction_for_roc_curve <- predict(rf_classifier,validation1[,-5],type="prob")
# Use pretty colours:
pretty_colours <- c("#F8766D","#00BA38","#619CFF")
# Specify the different classes
classes <- levels(validation1$Species)
# For each class
for (i in 1:3)
{
# Define which observations belong to class[i]
true_values <- ifelse(validation1[,5]==classes[i],1,0)
# Assess the performance of classifier for class[i]
pred <- prediction(prediction_for_roc_curve[,i],true_values)
perf <- performance(pred, "tpr", "fpr")
if (i==1)
{
plot(perf,main="ROC Curve",col=pretty_colours[i])
}
else
{
plot(perf,main="ROC Curve",col=pretty_colours[i],add=TRUE)
}
# Calculate the AUC and print it to screen
auc.perf <- performance(pred, measure = "auc")
print(auc.perf@y.values)
}
View(validation1)
validation1[,-5]
# ROC curve
prediction_for_roc_curve = predict(elastic, test_df, type="prob")
# prediction AUC
View(prediction_for_roc_curve)
library(ROCR)
#import the package
library(randomForest)
library(datasets)
data(iris)
# Perform training:
set.seed(17)
# Calculate the size of each of the data sets:
data_set_size <- floor(nrow(iris)/2)
# Generate a random sample of "data_set_size" indexes
indexes <- sample(1:nrow(iris), size = data_set_size)
# Assign the data to the correct sets
training <- iris[indexes,]
validation1 <- iris[-indexes,]
rf_classifier = randomForest(Species ~ ., data=training, ntree=100, mtry=2, importance=TRUE)
rf_classifier
prediction_for_table <- predict(rf_classifier,validation1[,-5])
table(observed=validation1[,5],predicted=prediction_for_table)
# Calculate the probability of new observations belonging to each class
# prediction_for_roc_curve will be a matrix with dimensions data_set_size x number_of_classes
prediction_for_roc_curve <- predict(rf_classifier,validation1[,-5],type="prob")
# Use pretty colours:
pretty_colours <- c("#F8766D","#00BA38","#619CFF")
# Specify the different classes
classes <- levels(validation1$Species)
# For each class
for (i in 1:3)
{
# Define which observations belong to class[i]
true_values <- ifelse(validation1[,5]==classes[i],1,0)
# Assess the performance of classifier for class[i]
pred <- prediction(prediction_for_roc_curve[,i],true_values)
perf <- performance(pred, "tpr", "fpr")
if (i==1)
{
plot(perf,main="ROC Curve",col=pretty_colours[i])
}
else
{
plot(perf,main="ROC Curve",col=pretty_colours[i],add=TRUE)
}
# Calculate the AUC and print it to screen
auc.perf <- performance(pred, measure = "auc")
print(auc.perf@y.values)
}
View(prediction_for_roc_curve)
View(test_label)
# ROC curve
# predictions for ROC curve
prediction_for_roc_curve = predict(elastic, test_df, type="prob")
# use pretty colors
pretty_colors <- c("#F8766D","#619CFF")
# specify the different tissue
classes <- levels(test_label$phenotype)
View(test_df)
# ROC curve
# predictions for ROC curve
prediction_for_roc_curve = predict(elastic, test_df, type="prob")
# use pretty colors
pretty_colors <- c("#F8766D","#619CFF")
# specify the different tissue
classes <- levels(test_label$phenotype)
# adopted from lab 4
for (i in 1:2) {
true_values = ifelse(test_label$phenotype==classes[i],1,0)
}
levels(test_label$phenotype)
levels(validation1$Species)
levels(test_label$phenotype)
as.factor(test_label$phenotype)
# ROC curve
# predictions for ROC curve
prediction_for_roc_curve = predict(elastic, test_df, type="prob")
# use pretty colors
pretty_colors = c("#F8766D","#619CFF")
# specify the different tissue
classes = levels(as.factor(test_label$phenotype))
# adopted from lab 4
for (i in 1:2) {
true_values = ifelse(as.factor(test_label$phenotype)==classes[i],1,0)
}
# ROC curve
# predictions for ROC curve
prediction_for_roc_curve = predict(elastic, test_df, type="prob")
# use pretty colors
pretty_colors = c("#F8766D","#619CFF")
# specify the different tissue
classes = levels(as.factor(test_label$phenotype))
# adopted from lab 4
for (i in 1:1) {
true_values = ifelse(as.factor(test_label$phenotype)==classes[i],1,0)
}
# ROC curve
# predictions for ROC curve
prediction_for_roc_curve = predict(elastic, test_df, type="prob")
# use pretty colors
pretty_colors = c("#F8766D","#619CFF")
# specify the different tissue
classes = levels(as.factor(test_label$phenotype))
# adopted from lab 4
for (i in 1:2) {
true_values = ifelse(as.factor(test_label$phenotype)==classes[i],1,0)
pred = prediction(prediction_for_roc_curve[,i], true_values)
}
View(pred)
# ROC curve
# predictions for ROC curve
prediction_for_roc_curve = predict(elastic, test_df, type="prob")
# use pretty colors
pretty_colors = c("#F8766D","#619CFF")
# specify the different tissue
classes = levels(as.factor(test_label$phenotype))
# adopted from lab 4
for (i in 1:2) {
true_values = ifelse(as.factor(test_label$phenotype)==classes[i],1,0)
pred = prediction(prediction_for_roc_curve[,i], true_values)
perf = performance(pred, "tpr", "fpr")
if (i==1) {
plot(perf, main="ROC Curve", col=pretty_colours[i])
}
else {
plot(perf, main="ROC Curve", col=pretty_colours[i], add=TRUE)
}
# Calculate the AUC and print it to screen
auc.perf = performance(pred, measure = "auc")
print(auc.perf@y.values)
}
# ROC curve
# predictions for ROC curve
prediction_for_roc_curve = predict(elastic, test_df, type="prob")
# specify the different tissue
classes = levels(as.factor(test_label$phenotype))
# adopted from lab 4
for (i in 1:2) {
true_values = ifelse(as.factor(test_label$phenotype)==classes[i],1,0)
pred = prediction(prediction_for_roc_curve[,i], true_values)
perf = performance(pred, "tpr", "fpr")
if (i==1) {
plot(perf, main="ROC Curve", col="green")
}
else {
plot(perf, main="ROC Curve", col="red", add=TRUE)
}
# Calculate the AUC and print it to screen
auc.perf = performance(pred, measure = "auc")
print(auc.perf@y.values)
}
legend("topright", inset=.05, title="Tissue Type",
c("Normal", "Tumor"), fill=terrain.colors(3), horiz=TRUE)
# ROC curve
# predictions for ROC curve
prediction_for_roc_curve = predict(elastic, test_df, type="prob")
# specify the different tissue
classes = levels(as.factor(test_label$phenotype))
# adopted from lab 4
for (i in 1:2) {
true_values = ifelse(as.factor(test_label$phenotype)==classes[i],1,0)
pred = prediction(prediction_for_roc_curve[,i], true_values)
perf = performance(pred, "tpr", "fpr")
if (i==1) {
plot(perf, main="ROC Curve", col="green")
}
else {
plot(perf, main="ROC Curve", col="red", add=TRUE)
}
# Calculate the AUC and print it to screen
auc.perf = performance(pred, measure = "auc")
print(auc.perf@y.values)
}
legend("topright", inset=.05, title="Tissue Type",
c("Normal", "Tumor"), fill=c("green", "red"), horiz=TRUE)
# ROC curve
# predictions for ROC curve
prediction_for_roc_curve = predict(elastic, test_df, type="prob")
# specify the different tissue
classes = levels(as.factor(test_label$phenotype))
# adopted from lab 4
for (i in 1:1) {
true_values = ifelse(as.factor(test_label$phenotype)==classes[i],1,0)
pred = prediction(prediction_for_roc_curve[,i], true_values)
perf = performance(pred, "tpr", "fpr")
if (i==1) {
plot(perf, main="ROC Curve", col="green")
}
else {
plot(perf, main="ROC Curve", col="red", add=TRUE)
}
# Calculate the AUC and print it to screen
auc.perf = performance(pred, measure = "auc")
print(auc.perf@y.values)
}
legend("topright", inset=.05, title="Tissue Type",
c("Normal", "Tumor"), fill=c("green", "red"), horiz=TRUE)
# ROC curve
# predictions for ROC curve
prediction_for_roc_curve = predict(elastic, test_df, type="prob")
# specify the different tissue
classes = levels(as.factor(test_label$phenotype))
# adopted from lab 4
for (i in 1:2) {
true_values = ifelse(as.factor(test_label$phenotype)==classes[i],1,0)
pred = prediction(prediction_for_roc_curve[,i], true_values)
perf = performance(pred, "tpr", "fpr")
if (i==1) {
plot(perf, main="ROC Curve", col="green")
}
else {
plot(perf, main="ROC Curve", col="red", add=TRUE)
}
# Calculate the AUC and print it to screen
auc.perf = performance(pred, measure = "auc")
print(auc.perf@y.values)
}
# add a legend to the graph
legend("topright", inset=.05, title="Tissue Type",
c("Normal", "Tumor"), fill=c("green", "red"), horiz=TRUE)
# ROC curve
# predictions for ROC curve
prediction_for_roc_curve = predict(elastic, test_df, type="prob")
# specify the different tissue
classes = levels(as.factor(test_label$phenotype))
# adopted from lab 4
for (i in 1:2) {
true_values = ifelse(as.factor(test_label$phenotype)==classes[i],1,0)
pred = prediction(prediction_for_roc_curve[,i], true_values)
perf = performance(pred, "tpr", "fpr")
if (i==1) {
plot(perf, main="ROC Curve", col="green")
}
else {
plot(perf, main="ROC Curve", col="red", add=TRUE)
}
# Calculate the AUC and print it to screen
auc.perf = performance(pred, measure = "auc")
print(auc.perf@y.values)
}
# add a legend to the graph
legend("topright", inset=.05, title="Tissue Type",
c("Normal", "Tumor"), fill=c("green", "red"), horiz=TRUE)
abline(a=0, b=1, lty=2, col="blue")
install.packages("ROCR")
install.packages("ROCR")
